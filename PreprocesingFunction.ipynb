{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segyio as sgy\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "def sample_from_sgy(monitor_file, baseline, n_shots=3, total_shots=8):\n",
    "    with sgy.open(monitor_file, strict=False, ignore_geometry=True) as data:\n",
    "        shot_ids = np.arange(int(total_shots/n_shots/2), int(total_shots - total_shots/n_shots/2), int(total_shots/n_shots))\n",
    "        if baseline is not None:\n",
    "            sample = np.expand_dims(get_shot(data, shot_ids[0]), axis=2) - np.expand_dims(get_shot(baseline, shot_ids[0]), axis=2)\n",
    "            for i in range(1, n_shots):\n",
    "                sample = np.concatenate((sample, np.expand_dims(get_shot(data, shot_ids[i]), axis=2) \\\n",
    "                                 - np.expand_dims(get_shot(baseline, shot_ids[i]), axis=2)), axis=2)\n",
    "        else:\n",
    "            sample = np.expand_dims(get_shot(data, shot_ids[0]), axis=2)\n",
    "            for i in range(1, n_shots):\n",
    "                sample = np.concatenate((sample, np.expand_dims(get_shot(data, shot_ids[i]), axis=2)), axis=2) \n",
    "            \n",
    "        # Normalize sample\n",
    "        if sample.std() != 0.0:\n",
    "            sample = sample - sample.mean()\n",
    "            sample = sample / sample.std()\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shot(f, i, traces_per_shot=128):\n",
    "    array = np.zeros((f.trace[0].shape[0], 0))\n",
    "    for j in range(0, int(traces_per_shot)):\n",
    "        array = np.hstack([array, np.expand_dims(f.trace[i*traces_per_shot+j], axis=1)])\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_from_model(monitor_file, baseline):\n",
    "    if baseline is not None:\n",
    "        return np.expand_dims(np.fromfile(monitor_file, dtype='float32').reshape((128, 128)) - baseline, axis=2)\n",
    "    else:\n",
    "        return np.expand_dims(np.fromfile(monitor_file, dtype='float32').reshape((128, 128)), axis=2)\n",
    "\n",
    "\n",
    "def get_features(base_dir, monitor_files, baseline, n_shots=3, total_shots=8):\n",
    "    samples = []\n",
    "    for im, monitor_file in enumerate(monitor_files):\n",
    "        print('Processing sample '+str(im)+'..', end='\\r')\n",
    "        samples.append(sample_from_sgy(os.path.join(base_dir, monitor_file), baseline, \n",
    "                                       n_shots=n_shots, total_shots=total_shots))\n",
    "    return np.stack(samples, axis=0)\n",
    "\n",
    "\n",
    "def get_targets(base_dir, monitor_files, baseline):\n",
    "    targets = []\n",
    "    for monitor_file in monitor_files:\n",
    "        targets.append(target_from_model(os.path.join(base_dir, monitor_file), baseline))\n",
    "    return np.stack(targets, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data_to_h5(filename, base_dir_seismic, monitor_files_seismic, base_dir_model, \n",
    "                                    monitor_files_model, baseline_seismic=None, baseline_model=None, \n",
    "                                            feature_shape=(751, 128, 3), target_shape=(128, 128, 1), n_shots=3, total_shots=8):\n",
    "    if len(monitor_files_model) != len(monitor_files_seismic):\n",
    "        raise ValueError('Need the same amount of models and seismic files')\n",
    "    with h5py.File(filename, 'w') as f:\n",
    "        maxshape_feature = (None,) + feature_shape\n",
    "        maxshape_target = (None,) + target_shape\n",
    "        features = f.create_dataset('features', (len(monitor_files_seismic),)+ feature_shape, maxshape=maxshape_feature, dtype=np.float32)\n",
    "        targets = f.create_dataset('targets', (len(monitor_files_seismic),)+ target_shape, maxshape=maxshape_target, dtype=np.float32)\n",
    "\n",
    "        for i in range(0, len(monitor_files_model)):\n",
    "            print('Writing sample '+str(i)+'..', end='\\r')\n",
    "            feature = sample_from_sgy(base_dir_seismic+'/'+monitor_files_seismic[i], baseline_seismic, n_shots=n_shots, total_shots=total_shots)\n",
    "            target = target_from_model(base_dir_model+'/'+monitor_files_model[i], baseline_model)\n",
    "            features[i] = feature\n",
    "            targets[i] = target\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_from_data_file(filename, batch_size=32):\n",
    "    with h5py.File(filename, 'r') as h5f:\n",
    "        shape = h5f['features'][0].shape\n",
    "        size = h5f['features'].shape[0]\n",
    "        start_index = 0\n",
    "        while True:\n",
    "            yield (h5f['features'][start_index:start_index+batch_size]), \\\n",
    "                    h5f['targets'][start_index:start_index+batch_size]\n",
    "            start_index = start_index + batch_size\n",
    "            # Reset generator when it reaches the end\n",
    "            if start_index + batch_size >= size:\n",
    "                start_index = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
